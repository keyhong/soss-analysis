{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5339077f-8215-4dcd-9479-25a9aba72bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aafff87-04a6-41e2-8b9a-2b06c7bcd76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/27 22:55:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Packages Loading\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# import query module\n",
    "sys.path.insert(0, '/home/icitydatahub/soss/queries')\n",
    "sys.path.insert(0, f\"{os.environ.get('SPARK_HOME')}/python\")\n",
    "\n",
    "# import, export Folder\n",
    "PREPROCESSING_DATA_FOLDER = '/home/icitydatahub/soss/01_safe_idex/PPRCS/Output'\n",
    "\n",
    "# ignore warning\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "\n",
    "# datetime\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# preprocessing function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# pyspark processing function\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# pyspark error exception\n",
    "from pyspark.sql.utils import ParseException\n",
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "# pyspark Postgresql query\n",
    "from safe_idex_query import QueryCollection\n",
    "from db_connector import SparkClass\n",
    "# 전역 변수 정의\n",
    "MONTH_PERIOD = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324eb4f-4041-4019-b89e-3896e02134d2",
   "metadata": {},
   "source": [
    "# Initializing today, gu_nm, gu_cd, query_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b82c72-6f5f-45e8-9819-3cd403fcd0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:55:25: Main-Process : Start\n",
      "22:55:25: -- spark load --\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new line str\n",
    "line_break = '\\n'\n",
    "\n",
    "# logging setting\n",
    "format = \"%(asctime)s: %(message)s\"\n",
    "logging.basicConfig(format=format, level=logging.INFO, datefmt=\"%H:%M:%S\")\n",
    "logging.info(\"Main-Process : Start\")\n",
    "start = time.time()\n",
    "\n",
    "logging.info(f\"-- spark load --{line_break}\")\n",
    "\n",
    "# today = sys.argv[1]\n",
    "today = '20230601'\n",
    "\n",
    "today_dt = datetime.strptime(today, '%Y%m%d')\n",
    "\n",
    "# 인천 구 딕셔너리 (구 이름 : 구 코드)\n",
    "gu_dict = {\n",
    "    'Jung-gu': 28110,\n",
    "    'Dong-gu': 28140,\n",
    "    'Michuhol-gu': 28177,\n",
    "    'Yeonsu-gu': 28185,\n",
    "    'Namdong-gu': 28200,\n",
    "    'Bupyeong-gu': 28237,\n",
    "    'Gyeyang-gu': 28245,\n",
    "    'Seo-gu': 28260,\n",
    "    'Ganghwa-gun': 28710,\n",
    "    'Ongjin-gun': 28720\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d34062f5-10bd-473c-8128-c03304cbc067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe_idex_query 인스턴스 생성\n",
    "query_obj = QueryCollection(today, 28110)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb95b8-ed3f-4d52-b281-83f6afde5a94",
   "metadata": {
    "tags": []
   },
   "source": [
    "# create_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4134ce04-edee-4c56-87c1-6b832040e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid() -> 'pandas.DataFrame':\n",
    "    \"\"\"01. 그리드 데이터를 생성하는 함수\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grid_dt_tm_df : pandas.DataFrame\n",
    "       공간격자 데이터 프레임\n",
    "    \"\"\"\n",
    "    \n",
    "    def import_grid() -> 'pandas.DataFrame':\n",
    "        \"\"\"01-(1) : 인천시 전체 공간격자 데이터를 불러오는 함수\"\"\"\n",
    "        \n",
    "        # 인천시 공간격자 데이터 불러오기\n",
    "        try:\n",
    "            grid_df = query_obj.get_ic_pcel_stdr_info_sql()\n",
    "            if not grid_df.take(1):\n",
    "                logging.error(\"Fucntion : get_ic_pcel_stdr_info_sql() 쿼리로 가져오는 데이터가 없습니다\")\n",
    "        except ParseException as e:\n",
    "            logging.error(f'ParseException : {e}')\n",
    "        except Py4JJavaError as e:\n",
    "            logging.error(f'Py4JJavaError : {e}')\n",
    "        else:\n",
    "            grid_df = grid_df.toPandas()\n",
    "\n",
    "        # 메모리 사용량 감소를 위해 데이터 타입을 object에서 int로 변경\n",
    "        grid_df = grid_df.astype({'admd_cd': 'UInt32', 'row_no': 'UInt16', 'clm_no': 'UInt16'})\n",
    "\n",
    "        return grid_df\n",
    "\n",
    "    def create_date(grid_df: 'pandas.DataFrame') -> 'pandas.DataFrame':\n",
    "        \"\"\"01-(2) : 그리드 데이터에 일자를 생성하는 함수\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grid_df : pandas.DataFrame\n",
    "           인천시 전체 그리드 데이터에서 최근 MONTH_PERIOD개월 간 유동인구가 포착된 그리드만 분류하여 추출한 데이터\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        grid_df : pandas.DataFrame\n",
    "           그리드에 일자를 생성한 데이터 프레임\n",
    "        \"\"\"\n",
    "\n",
    "        start_date = (today_dt - relativedelta(months=MONTH_PERIOD)).strftime('%Y%m%d')\n",
    "        end_date = (today_dt - relativedelta(days=1)).strftime('%Y%m%d')\n",
    "\n",
    "        # 시작날짜와 끝날짜 사이의 모든 일자 리스트 생성\n",
    "        days_arr = pd.date_range(start=start_date, end=end_date).strftime(\"%Y%m%d\").tolist()\n",
    "\n",
    "        # 일자별로 공간격자를 생성한 데이터를 담을 합계 데이터프레임\n",
    "        grid_dt_df = pd.DataFrame()\n",
    "\n",
    "        # 일자 확장\n",
    "        for date in days_arr:\n",
    "            grid_df['stdr_de'] = date\n",
    "            grid_dt_df = pd.concat([grid_dt_df, grid_df], ignore_index=True)\n",
    "        \n",
    "        logging.info(f'일자 확장 후 : {grid_dt_df.shape[0]:,}')\n",
    "\n",
    "        # 컬럼 재정렬\n",
    "        grid_dt_df = grid_dt_df[['stdr_de', 'grid_id', 'mtr_no', 'admd_cd', 'row_no', 'clm_no']]\n",
    "\n",
    "        return grid_dt_df\n",
    "\n",
    "    def create_time(grid_dt_df: 'pandas.DataFrame') -> 'pandas.DataFrame':\n",
    "        \"\"\"01-(4) : 그리드 데이터에 시간을 생성하는 함수\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grid_dt_df : pandas.DataFrame\n",
    "           그리드에 일자를 생성한 데이터 프레임\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        grid_dt_df : pandas.DataFrame\n",
    "           그리드에 시간을 생성한 데이터 프레임\n",
    "        \"\"\"\n",
    "\n",
    "        # 시간별로 공간격자를 생성한 데이터를 담을 합계 데이터프레임\n",
    "        grid_dt_tm_df = pd.DataFrame() \n",
    "        \n",
    "        # 시간 확장\n",
    "        for tm in range(0, 24):\n",
    "            grid_dt_df['stdr_tm'] = tm\n",
    "            grid_dt_tm_df = pd.concat([grid_dt_tm_df, grid_dt_df], ignore_index=True)\n",
    "        \n",
    "        logging.info(f'시간 확장 후 : {grid_dt_df.shape[0]:,}')\n",
    "        \n",
    "        # type casting\n",
    "        grid_dt_tm_df['stdr_tm'] = grid_dt_tm_df['stdr_tm'].astype('UInt8')\n",
    "        \n",
    "        # 컬럼 재정렬\n",
    "        grid_dt_tm_df = grid_dt_tm_df[['stdr_de', 'stdr_tm', 'grid_id', 'mtr_no', 'admd_cd', 'row_no', 'clm_no']]\n",
    "\n",
    "        return grid_dt_tm_df\n",
    "\n",
    "    # 01-(1) 그리드 데이터 생성하는 함수\n",
    "    grid_df = import_grid()\n",
    "\n",
    "    # 01-(2) 공간격자 데이터에 일자 변수 생성\n",
    "    grid_dt_df = create_date(grid_df)    \n",
    "    \n",
    "    # 01-(3) 공간격자 데이터에 시간 변수 생성\n",
    "    grid_dt_tm_df = create_time(grid_dt_df)\n",
    "    \n",
    "    return grid_dt_tm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ee6e9-4f73-4b47-97c9-dd222a0c2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. 인천 공간격자 데이터 로딩\n",
    "grid_dt_tm_df = create_grid()\n",
    "logging.info('-- create_grid() 종료 --')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b24ed8e-8ba9-48df-8b2a-912d9f45d3ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# merge_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36990e-10c4-42e6-be5e-487f11fd3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_report(grid_dt_tm_df: 'pandas.DataFrame') -> 'pandas.DataFrame':\n",
    "    \"\"\"02. 경찰 신고 접수 데이터 병합하는 함수\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    grid_dt_tm_df : pandas.DataFrame\n",
    "       공간격자 데이터 프레임\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    report_merge_df : pandas.DataFrame\n",
    "        경찰 신고 접수 데이터 병합한 데이터프레임\n",
    "    \"\"\"\n",
    "    \n",
    "    def import_report() -> 'pandas.DataFrame':\n",
    "        \"\"\"02-(1) : 경찰신고접수 데이터를 가져오는 함수\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        report_df : pandas.DataFrame\n",
    "           경찰청 신고 데이터 프레임  \n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            start_month = (today_dt - relativedelta(months=MONTH_PERIOD)).strftime('%m')\n",
    "            end_month = (today_dt - relativedelta(days=1)).strftime('%m')\n",
    "            report_df = query_obj.get_npa_dcr_rcp_sql(start_month, end_month)\n",
    "            \n",
    "            if not report_df.take(1):\n",
    "                logging.error(\"Fucntion : get_npa_dcr_rcp_sql() 쿼리로 가져오는 데이터가 없습니다\")\n",
    "        except ParseException as e:\n",
    "            logging.error(f'ParseException : {e}')\n",
    "        except Py4JJavaError as e:\n",
    "            logging.error(f'Py4JJavaError : {e}')\n",
    "        else:\n",
    "            report_df = report_df.toPandas()\n",
    "        \n",
    "        report_df['stdr_de'] = report_df['stdr_de'].str.slice_replace(0, 4, today[0:4])\n",
    "        \n",
    "        # type casting\n",
    "        report_df = report_df.astype({'stdr_tm': 'UInt8'})\n",
    "\n",
    "        return report_df\n",
    "\n",
    "    def classify_important_reception(report_df: 'pandas.DataFrame') -> 'pandas.DataFrame':\n",
    "        \"\"\"02-(2) : 중범죄 신고를 분별하는 함수\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        report_df : pandas.DataFrame\n",
    "           경찰청 신고 데이터 프레임\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        important_report_df : pandas.DataFrame\n",
    "           중요 신고 데이터를 분별한 데이터 프레임   \n",
    "           \n",
    "        \"\"\"\n",
    "\n",
    "        # 중범죄 사건 리스트 \n",
    "        important_report = [\n",
    "            '가정폭력', '강도', '공갈', '기타형사범',\n",
    "            '납치감금', '데이트폭력', '변사자', '사망·대형사고',\n",
    "            '살인', '성폭력', '스토킹', '실종(실종아동 등)',\n",
    "            '아동학대(가정내)', '아동학대(기타)', '인피도주', '자살',\n",
    "            '재물손괴', '절도', '주거침입', '치기',\n",
    "            '폭력', '풍속영업', '학교폭력', '협박'\n",
    "        ]\n",
    "\n",
    "        # 사건종별명(incd_ass_nm)이 중요 사건 리스트에 포함 되면 1, 그렇지 않으면 0 라벨링 부여\n",
    "        report_df['importance'] = np.where(report_df['incd_ass_nm'].isin(important_report), np.uint8(1), np.uint8(0))\n",
    "\n",
    "        # \"일자별-시간별-행렬번호별\" 로 그룹핑하여, 사건 개수와 중요사건 합계 반환\n",
    "        important_report_df = report_df.groupby(by=['stdr_de', 'stdr_tm', 'mtr_no'], as_index=False).agg(report_cnt=('mtr_no', 'count'), impt_report_cnt=('importance', 'sum'))\n",
    "        \n",
    "        # type casting\n",
    "        important_report_df = important_report_df.astype({'report_cnt': 'UInt16', 'impt_report_cnt': 'UInt16'})\n",
    "\n",
    "        return important_report_df\n",
    "\n",
    "    def classify_urgent_reception(report_df: 'pandas.DataFrame') -> 'pandas.DataFrame':\n",
    "        \"\"\"02-(3) : 긴급한 신고를 분별하는 함수\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        report_df : pandas.DataFrame\n",
    "           경찰청 신고 데이터 프레임\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        urgent_report_df : pandas.DataFrame\n",
    "           긴급 신고 데이터를 분별한 데이터 프레임\n",
    "        \"\"\"\n",
    "\n",
    "        # 접수유형이 (C0, C1)인 데이터 => 긴급 신고 데이터만 추출\n",
    "        urgent_report = ['C0', 'C1']\n",
    "        report_df = report_df[report_df['incd_emr_cd'].isin(urgent_report)]\n",
    "        report_df['isUrgent'] = 1\n",
    "\n",
    "        # \"일자-시간-행렬번호-사건종별명\" 으로 피벗하여, 접수유형별 개수 반환 (긴급 사건)\n",
    "        urgent_report_df = report_df.pivot_table(\n",
    "            index=['stdr_de', 'stdr_tm', 'mtr_no'],\n",
    "            columns='incd_emr_cd',\n",
    "            values='isUrgent',\n",
    "            aggfunc='count',\n",
    "            fill_value=0\n",
    "        ).reset_index()\n",
    "\n",
    "        # 피벗의 대상열 제거\n",
    "        urgent_report_df.rename_axis(None, axis=1, inplace=True)\n",
    "\n",
    "        # type cast \n",
    "        urgent_report_df = urgent_report_df.astype({'C0': 'UInt16', 'C1': 'UInt16'})\n",
    "\n",
    "        return urgent_report_df\n",
    "    \n",
    "    def create_hazard_score(impt_urgent_report_df: 'pandas.DataFrame') -> 'pandas.DataFrame':\n",
    "        \"\"\"02-(4) : 신고 점수 생성 및 신고 발생 종류(중요/긴급)에 따라 가중치를 부여하는 함수\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        impt_urgent_report_df : pandas.DataFrame\n",
    "           분별한 중요신고와 긴급신고를 결합한 데이터프레임\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        hazard_df : pandas.DataFrame\n",
    "           위해지표를 생성한 데이터프레임\n",
    "           \n",
    "        \"\"\"\n",
    "        # 위해지표를 생성할 데이터프레임 생성\n",
    "        hazard_df = impt_urgent_report_df.copy()\n",
    "        \n",
    "        # 신고점수(위해지표) 생성\n",
    "        hazard_df['hazard_score'] =  hazard_df['report_cnt'].copy()\n",
    "\n",
    "        # 개별 그리드에서 발생한 모든 신고 중에서 중요 신고가 절반이상 일 때 5배 가중치 부여\n",
    "        hazard_df['hazard_score'] = np.where(hazard_df['impt_report_cnt'] / hazard_df['report_cnt'] > 0.5, hazard_df['hazard_score'] * 5, hazard_df['hazard_score'])\n",
    "\n",
    "        # 개별 그리드에서 발생한 모든 신고 중에서 긴급 신고(C0, C1)가 있다면 각각 30배, 10배 가중치 부여\n",
    "        hazard_df['hazard_score'] = np.where(hazard_df['C0'] > 0, hazard_df['hazard_score'] * 30, hazard_df['hazard_score'])\n",
    "        hazard_df['hazard_score'] = np.where((hazard_df['C0'] == 0) & (hazard_df['C1'] > 0), hazard_df['hazard_score'] * 10, hazard_df['hazard_score'])\n",
    "\n",
    "        # 중요 사건이면서 긴급신고인 사건에 라벨링 컬럼 추가\n",
    "        hazard_df['priority'] = np.where((hazard_df['impt_report_cnt'] / hazard_df['report_cnt'] > 0.5) & (hazard_df['C0'] > 0), 1, 0)\n",
    "        \n",
    "        # type cast\n",
    "        hazard_df = hazard_df.astype({'hazard_score': float, 'priority': 'UInt8'})\n",
    "\n",
    "        # 컬럼 정리\n",
    "        hazard_df.drop(['report_cnt', 'impt_report_cnt', 'C0', 'C1'], axis=1, inplace=True)\n",
    "\n",
    "        return hazard_df\n",
    "\n",
    "    def expand_time(hazard_df: 'pandas.DataFrame') -> 'pandas.DataFrame':\n",
    "        \"\"\"02-(5) : 신고 데이터의 시간을 확장하여, 동일 신고 발생 그리드의 1시간 전후 시간내 가중 점수를 더해주는 함수\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hazard_df : pandas.DataFrame\n",
    "           위해지표를 생성한 데이터프레임\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        merge_df : pandas.DataFrame\n",
    "           그리드 데이터와 경찰청 신고 데이터를 병합한 데이터 프레임\n",
    "        \"\"\"\n",
    "\n",
    "        # (1) 우선 사건 라벨링 테이블 추출 (report 데이터 grouping시, 라벨 컬럼은 삭제되기 때문에 이후 다시 재결합)\n",
    "        priority_df = hazard_df[hazard_df['priority']==1].copy()\n",
    "        priority_df = priority_df[['stdr_de', 'stdr_tm', 'mtr_no', 'priority']]\n",
    "\n",
    "        # (2) 신고 데이터 시간 확장 : (-1 시간 ~ 현재 ~ +1시간)의 데이터에 가중 신고점수 부여\n",
    "        report_expand_df = pd.DataFrame()\n",
    "\n",
    "        for tm in range(-1, 2):\n",
    "\n",
    "            hazard_copy_df = hazard_df.copy()\n",
    "\n",
    "            hazard_copy_df['stdr_tm'] = hazard_copy_df['stdr_tm'] + tm\n",
    "            hazard_copy_df['hazard_score'] = hazard_copy_df['hazard_score'] * (1 - (0.5 * abs(tm)))\n",
    "\n",
    "            report_expand_df = pd.concat([report_expand_df, hazard_copy_df], ignore_index=True)\n",
    "            \n",
    "        # (3) 추가해준 데이터에 대한 일자, 시간 재조정 (시간을 더해주거나 감소시켜, 일자가 바뀌는 경우)\n",
    "        \n",
    "        # 날짜 형식으로 데이터 타입 변환\n",
    "        report_expand_df['stdr_de'] = report_expand_df['stdr_de'].astype('datetime64[ns]') \n",
    "\n",
    "        # TM이 0보다 작으면, 일자는 하루 감소, 시간은 24 증가\n",
    "        report_expand_df['stdr_de'] = np.where(report_expand_df['stdr_tm'] < 0, report_expand_df['stdr_de'] - timedelta(days=1), report_expand_df['stdr_de'])\n",
    "        report_expand_df['stdr_tm'] = np.where(report_expand_df['stdr_tm'] < 0, report_expand_df['stdr_tm'] + 24, report_expand_df['stdr_tm'])\n",
    "\n",
    "        # TM이 23보다 크면, 일자는 하루 증가, 시간은 24 감소\n",
    "        report_expand_df['stdr_de'] = np.where(report_expand_df['stdr_tm'] > 23, report_expand_df['stdr_de'] + timedelta(days=1), report_expand_df['stdr_de'])\n",
    "        report_expand_df['stdr_tm'] = np.where(report_expand_df['stdr_tm'] > 23, report_expand_df['stdr_tm'] - 24, report_expand_df['stdr_tm'])\n",
    "\n",
    "        # type casting\n",
    "        report_expand_df['stdr_de'] = report_expand_df['stdr_de'].apply(lambda x: datetime.strftime(x, '%Y%m%d'))\n",
    "        report_expand_df['stdr_tm'] = report_expand_df['stdr_tm'].astype('UInt8')\n",
    "\n",
    "        # (4) 신고점수 합계\n",
    "        report_expand_df = report_expand_df.groupby(by=['stdr_de', 'stdr_tm', 'mtr_no'], as_index=False).agg(hazard_score=('hazard_score', 'sum'))\n",
    "\n",
    "        # (5) 시작일과 종료일을 벗어나는 데이터 제거\n",
    "        start_date = (today_dt - relativedelta(months=MONTH_PERIOD)).strftime('%Y%m%d')\n",
    "        end_date = (today_dt - relativedelta(days=1)).strftime('%Y%m%d')\n",
    "\n",
    "        condition = (start_date <= report_expand_df['stdr_de']) & (report_expand_df['stdr_de'] <= end_date)\n",
    "        report_expand_df = report_expand_df[condition]\n",
    "\n",
    "        # (6) 우선사건 라벨링 결합\n",
    "        report_expand_df = report_expand_df.merge(priority_df, how='left', on=['stdr_de', 'stdr_tm', 'mtr_no'])\n",
    "        report_expand_df['priority'].fillna(1, inplace=True)\n",
    "\n",
    "        return report_expand_df\n",
    "\n",
    "    # 02-(1) 신고 데이터를 불러오는 함수\n",
    "    report_df = import_report()        \n",
    "        \n",
    "    # 02-(2) 중범죄 신고를 분별하는 함수\n",
    "    important_report_df = classify_important_reception(report_df)\n",
    "\n",
    "    # 02-(3) 긴급한 신고를 분별하는 함수\n",
    "    urgent_report_df = classify_urgent_reception(report_df)\n",
    "\n",
    "    # 중범죄 신고, 긴급신고 테이블 결합\n",
    "    impt_urgent_report_df = important_report_df.merge(urgent_report_df, how='outer', on=['stdr_de', 'stdr_tm', 'mtr_no'])\n",
    "    impt_urgent_report_df.fillna(value={'C0': 0, 'C1': 0}, inplace=True)\n",
    "\n",
    "    # 02-(4) 위해지표 생성 및 신고 발생 종류(중요/긴급)에 따라 가중점수를 부여하는 함수\n",
    "    hazard_df = create_hazard_score(impt_urgent_report_df)\n",
    "\n",
    "    # 02-(5) 1시간 전후의 시간에 따라 신고 점수를 확장하는 함수\n",
    "    report_expand_df = expand_time(hazard_df)\n",
    "\n",
    "    # 02-(6) 공간격자 데이터와 신고 데이터 병합\n",
    "    report_merge_df = grid_dt_tm_df.merge(report_expand_df, how='left', on=['stdr_de', 'stdr_tm', 'mtr_no'])\n",
    "    report_merge_df.fillna(value={'hazard_score': 0, 'priority': 0}, inplace=True)\n",
    "\n",
    "    return report_merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23190d49-42fd-4412-b36e-c30843062357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. 경찰 신고 접수 데이터 병합\n",
    "report_merge_df = merge_report(grid_dt_tm_df)\n",
    "logging.info('-- merge_report() 종료 --')\n",
    "del grid_dt_tm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afcf776-726e-4acc-9988-dd6a4e25393e",
   "metadata": {},
   "source": [
    "# expand_grid_scrore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50732e9d-e623-4714-a32c-b052c154cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_grid_scrore(report_merge_df: 'pandas.DataFrame') -> 'pandas.DataFrame':\n",
    "    \"\"\"03. 핵심 신고 위치 주변으로 핵심 신고 발생 그리드의 가중치를 덧셈 연산해주는 함수\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    report_merge_df : pandas.DataFrame\n",
    "        경찰 신고 접수 데이터 병합한 데이터프레임\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    grid_expand_df : pandas.DataFrame\n",
    "        공간격자 확장한 데이터프레임\n",
    "    \"\"\"\n",
    "    \n",
    "    def expand_grid(priority_df: 'pandas.DataFrame', weight: float=0.3, expand_cnt: int=2) -> 'pandas.DataFrame':\n",
    "        \"\"\"03-(1) : 신고 발생 데이터를 주변 그리드로 공간 확장하는 함수\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        priority_df : pandas.DataFrame\n",
    "           공간 확장 시키려고 하는 데이터프레임\n",
    "        weight : float\n",
    "           공간 확장시마다 신고점수에 곱하는 가중치 점수의 감소분\n",
    "        expand_cnt : int\n",
    "           상하 좌우 및 대각선으로 확장하려는 공간격자의 범위 수\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        priority_expand_df : pandas.DataFrame\n",
    "            우선 순위 공간격자를 확장한 데이터프레임\n",
    "        \"\"\"\n",
    "\n",
    "        priority_expand_df = pd.DataFrame()\n",
    "\n",
    "        for _, row in priority_df.iterrows():\n",
    "\n",
    "            # 1칸 ~ 상하 좌우 및 대각선 총 expand_cnt칸 확장 개수만큼 돌면서 주변 격자의 번호 조회\n",
    "            for expand_num in range(1, expand_cnt+1): \n",
    "\n",
    "                priority_expand_part_df = pd.DataFrame(\n",
    "                    print_mtr_no(row['row_no'], row['clm_no'], expand_num),\n",
    "                    columns=['mtr_no']\n",
    "                )\n",
    "\n",
    "                priority_expand_part_df['stdr_de'] = row['stdr_de']\n",
    "                priority_expand_part_df['stdr_tm'] = row['stdr_tm']\n",
    "                priority_expand_part_df['expand_hazard_score'] = row['hazard_score'] * np.float64(1 - (expand_num * weight))\n",
    "\n",
    "                priority_expand_df = pd.concat([priority_expand_df, priority_expand_part_df], ignore_index=True)\n",
    "\n",
    "        priority_expand_df = priority_expand_df.groupby(['stdr_de', 'stdr_tm', 'mtr_no'], as_index=False)['expand_hazard_score'].sum()\n",
    "\n",
    "        return priority_expand_df\n",
    "\n",
    "    def print_mtr_no(row_no: int, clm_no: int, expand_num: int) -> 'numpy.ndarray':\n",
    "        \"\"\"03-(2) : 확장하려는 공간격자의 행렬 번호를 반환하는 함수\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        row_no : int\n",
    "           공간격자의 행 번호\n",
    "        clm_no : int\n",
    "           공간격자의 열 번호\n",
    "        expand_cnt : int\n",
    "           확장 하려는 공간의 범위 수\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        target_grid_arr : numpy.ndarray\n",
    "           인풋한 행번호와 열번호를 기준으로 distnace만큼 확장한 그리드들의 행렬번호를 모아둔 어레이\n",
    "        \"\"\"\n",
    "\n",
    "        dxy = [ row_no-expand_num, row_no+expand_num, clm_no-expand_num, clm_no+expand_num ]\n",
    "\n",
    "        row_range = [ row for row in range(row_no - expand_num, row_no + expand_num+1) ]\n",
    "        clm_range = [ clm for clm in range(clm_no - expand_num, clm_no + expand_num+1) ]\n",
    "\n",
    "        grid_list1 = [\",\".join([str(x_id), str(y)]) for x_id in dxy[0:2] for y in clm_range]\n",
    "        grid_list2 = [\",\".join([str(x), str(y_id)]) for y_id in dxy[3:5] for x in row_range]\n",
    "\n",
    "        target_grid_arr = np.unique(np.array(grid_list1 + grid_list2))\n",
    "\n",
    "        return target_grid_arr\n",
    "\n",
    "    # 공간확장 대상 : 신고가 발생하고, 최우선(중요사건 & 긴급신고) 조건을 충족하는 데이터만 추출\n",
    "    priority_df = report_merge_df[report_merge_df['priority']==1].copy()\n",
    "    priority_df = priority_df[['stdr_de', 'stdr_tm', 'row_no', 'clm_no', 'hazard_score']]\n",
    "\n",
    "    # 공간을 확장하여 신고점수를 산출한 데이터프레임\n",
    "    priority_expand_df = expand_grid(priority_df, weight=0.3, expand_cnt=2)\n",
    "    grid_expand_df = report_merge_df.merge(priority_expand_df, how='left', on=['stdr_de', 'stdr_tm', 'mtr_no'])\n",
    "    \n",
    "    # 테이블 병합 후 expand_hazard_score 결측값 채우기\n",
    "    grid_expand_df['expand_hazard_score'].fillna(0, inplace=True)\n",
    "    \n",
    "    # 위해지표 합계 계산\n",
    "    grid_expand_df['hazard_score'] = grid_expand_df['hazard_score'] + grid_expand_df['expand_hazard_score']\n",
    "\n",
    "    # 불필요 컬럼 제거\n",
    "    grid_expand_df.drop(['mtr_no', 'row_no', 'clm_no', 'priority', 'expand_hazard_score'], axis=1, inplace=True)\n",
    "    \n",
    "    return grid_expand_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c112b-b06a-4afc-9ec1-c5f03188e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_expand_df = expand_grid_scrore(report_merge_df.loc[:7951, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795fd6c8-e440-4ba1-9784-b7b0b7515947",
   "metadata": {},
   "source": [
    "# merge_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c9b89-260a-4dc6-9b5b-3fa4aab6efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_weather(grid_expand_df: 'pandas.DataFrame') -> 'pandas.DataFrame':\n",
    "    \"\"\"04. 날씨 데이터를 병합하는 함수\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    grid_expand_df : pandas.DataFrame\n",
    "       날씨 데이터를 병합할 데이터 프레임\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tmp_wether_merge_df : pandas.DataFrame\n",
    "       날씨 데이터를 병합한 데이터 프레임\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    실시간 날씨 데이터를 병합하고, 결측값은 지난년도의 날씨 데이터를 병합한다. 그래도 결측값이 있다면 같은 격자, 같은 시간, 다른 날짜의 선형 값으로 보간\n",
    "    \n",
    "    \"\"\"   \n",
    "    \n",
    "    def import_wether() -> 'pandas.DataFrame':\n",
    "        \"\"\"04-(1) : 일배치 날씨 데이터를 로딩하는 함수\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weather_df : pandas.DataFrame\n",
    "           일배치의 날씨 데이터\n",
    "        \"\"\"\n",
    "\n",
    "        # 2개월 전 ~ 어제(매월 1일 기준)의 날씨 데이터 로딩\n",
    "        try:\n",
    "            start_date = (today_dt - relativedelta(months=MONTH_PERIOD)).strftime('%Y%m%d')\n",
    "            end_date = (today_dt - relativedelta(days=1)).strftime('%Y%m%d')\n",
    "\n",
    "            weather_df = query_obj.get_ic_wth_frcs_sql(start_date, end_date)\n",
    "            \n",
    "        except ParseException as e:\n",
    "            logging.error(f'ParseException : {e}')\n",
    "        except Py4JJavaError as e:\n",
    "            logging.error(f'Py4JJavaError : {e}')\n",
    "        else:\n",
    "            weather_df = weather_df.toPandas()\n",
    "        \n",
    "        weather_df = weather_df.astype({'stdr_tm': 'UInt8', 'admd_cd': 'UInt32'})\n",
    "\n",
    "        return weather_df\n",
    "\n",
    "    def import_tmp_weather() -> 'pandas.DataFrame':\n",
    "        \"\"\"04-(2) : 보간용 날씨 데이터를 로딩하는 함수\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tmp_weather_df : pandas.DataFrame\n",
    "           보간용 날씨 데이터\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            tmp_weather_df = query_obj.get_tmp_ic_wth_frcs_sql()\n",
    "            if not tmp_weather_df.take(1):\n",
    "                logging.error(\"Fucntion : get_tmp_ic_wth_frcs_sql() 쿼리로 가져오는 데이터가 없습니다\")\n",
    "        except ParseException as e:\n",
    "            logging.error(f'ParseException : {e}')\n",
    "        except Py4JJavaError as e:\n",
    "            logging.error(f'Py4JJavaError : {e}')\n",
    "        else:\n",
    "            tmp_weather_df = tmp_weather_df.toPandas()\n",
    "            tmp_weather_df['stdr_de'] = tmp_weather_df['stdr_de'].str.slice_replace(0, 4, today[0:4])\n",
    "            \n",
    "            tmp_weather_df = tmp_weather_df.astype({'stdr_tm': 'UInt8', 'admd_cd': 'UInt32'})\n",
    "            tmp_weather_df.rename(columns={'hd_val': 'sub_hd_val', 'atp_val': 'sub_atp_val'}, inplace=True)\n",
    "\n",
    "        return tmp_weather_df\n",
    "\n",
    "    # 날씨 데이터 불러와서 결합하기\n",
    "    weather_df = import_wether()\n",
    "    weather_merge_df = grid_expand_df.merge(weather_df, how='left', on=['stdr_de', 'stdr_tm', 'admd_cd'])\n",
    "\n",
    "    # 결합된 날씨 데이터에 N/A값이 있다면, 2022년 데이터를 활용해 빈 데이터 보간\n",
    "    tmp_wether_df = import_tmp_weather()\n",
    "    tmp_wether_merge_df = weather_merge_df.merge(tmp_wether_df, how='left', on=['stdr_de', 'stdr_tm', 'admd_cd'])\n",
    "\n",
    "    tmp_wether_merge_df['atp_val'] = np.where(tmp_wether_merge_df['atp_val'].isnull(), tmp_wether_merge_df['sub_atp_val'], tmp_wether_merge_df['atp_val'])\n",
    "    tmp_wether_merge_df['hd_val'] = np.where(tmp_wether_merge_df['hd_val'].isnull(), tmp_wether_merge_df['sub_hd_val'], tmp_wether_merge_df['hd_val'])\n",
    "    tmp_wether_merge_df.drop(['sub_hd_val', 'sub_atp_val'], axis=1, inplace=True)\n",
    "\n",
    "    # 대체 데이터를 결합해도 N/A 값이 있다면 정렳후 결측값을 선형보간법으로 채우기\n",
    "    tmp_wether_merge_df.sort_values(by=['admd_cd', 'stdr_tm', 'stdr_de'], inplace=True)\n",
    "    tmp_wether_merge_df['atp_val'] = tmp_wether_merge_df['atp_val'].interpolate()\n",
    "    tmp_wether_merge_df['hd_val'] = tmp_wether_merge_df['hd_val'].interpolate()\n",
    "    \n",
    "    return tmp_wether_merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a829987-11cd-45aa-91b8-854f4d5c9d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. 인천 날씨 예보 데이터를 병합\n",
    "wether_merge_df = merge_weather(grid_expand_df)\n",
    "logging.info('-- merge_weather() 종료 --')\n",
    "del grid_expand_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e62d64c-4e39-4c1b-a575-128ac8e145da",
   "metadata": {},
   "source": [
    "# merge_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1223dcf-4373-4a89-ad74-efc7750b6172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_pop() -> 'pandas.DataFrame':\n",
    "    ''' 05-(1) 유동인구 통계 데이터 불러오기\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pop_df : pandas.DataFrame\n",
    "        유동인구 데이터프레임\n",
    "\n",
    "    '''\n",
    "    try:\n",
    "\n",
    "        start_month = (today_dt - relativedelta(months=MONTH_PERIOD)).strftime('%m')\n",
    "        end_month = (today_dt - relativedelta(days=1)).strftime('%m')\n",
    "\n",
    "        months = (start_month, end_month)\n",
    "\n",
    "        sdf_lst = []\n",
    "\n",
    "        for month in months:\n",
    "            pop_df_part = query_obj.get_dw_ic_pcell_tmzn_fpop_sql(month)\n",
    "\n",
    "            # 통계용 데이터 요일 컬럼 생성\n",
    "            pop_df_part = pop_df_part.withColumn('day_nm', F.to_date(F.unix_timestamp('stdr_de', 'yyyyMMdd').cast(\"timestamp\")))\n",
    "            pop_df_part = pop_df_part.withColumn('day_nm', F.date_format('day_nm', 'E'))\n",
    "\n",
    "            from itertools import chain\n",
    "            day_dict = { 'Mon': 0, 'Tue': 1, 'Wed': 2, 'Thu': 3, 'Fri': 4, 'Sat': 5, 'Sun': 6 }\n",
    "            mapping = F.create_map([F.lit(x) for x in chain(*day_dict.items())])                \n",
    "            pop_df_part = pop_df_part.withColumn('day_nm', mapping[pop_df_part['day_nm']])\n",
    "\n",
    "            sdf_lst.append(pop_df_part)\n",
    "    except ParseException as e:\n",
    "        logging.error(f'ParseException : {e}')\n",
    "    except Py4JJavaError as e:\n",
    "        logging.error(f'Py4JJavaError : {e}')            \n",
    "    else:\n",
    "        # 두 테이블 세로병합\n",
    "        pop_df = sdf_lst[0].unionAll(sdf_lst[1])\n",
    "        \n",
    "        # 그룹핑핑 값으로 평균 취약지표 산출\n",
    "        pop_df = pop_df.groupby(['grid_id', 'stdr_tm', 'day_nm']).agg(F.mean('weak_score').alias('weak_score'))                \n",
    "\n",
    "        pop_df = pop_df.toPandas()\n",
    "        pop_df['stdr_tm'] = pop_df['stdr_tm'].astype('UInt8')\n",
    "\n",
    "        return pop_df\n",
    "\n",
    "# 유동인구 데이터 로딩\n",
    "pop_df = import_pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e876f2-b263-4847-9eec-dea7ff5f02ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wether_merge_df에 요일 컬럼 추가\n",
    "wether_merge_df['day_nm'] = pd.to_datetime(wether_merge_df['stdr_de']).dt.weekday\n",
    "\n",
    "pop_merge_df = wether_merge_df.merge(pop_df, how='left', on=['grid_id', 'stdr_tm', 'day_nm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51dc19-48c4-41c9-a1e4-41179c794238",
   "metadata": {},
   "source": [
    "# down_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa316d-c589-4860-b7ec-fb0a80bbc4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sampling(merge_df: 'pandas.DataFrame', target_col: str, nrows: int=1_000_000, target_percent: float=0.25) -> 'pandas.DataFrame':\n",
    "    \"\"\"06. 위해지표 데이터를 다운 샘플링하여 학습 데이터의 밸런스을 맞춰주는 함수\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    pop_merge_df : pandas.DataFrame\n",
    "       언더샘플링을 적용할 유동인구 병합 데이터프레임\n",
    "    target_col : str\n",
    "       언더샘플링의 대상이 기준 대상이 되는 컬럼의 이름\n",
    "    nrows\n",
    "       언더샘플링하여 최종적으로 추출하려는 행의 개수\n",
    "    target_percent: float\n",
    "       전체 행의 개수에서 맞추려고 하는 신고 발생 데이터의 비율\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sampling_df: pandas.DataFrame\n",
    "       언더샘플링이 적용되어 추출된 데이터 프레임. nrows개 중 target_percent만큼 신고 데이터가 들어있음\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    언더샘플링을 시행시 신고가 발생하지 않았던 데이터가 너무 적어 target_percent 만큼 비율을 채우지 못할 경우, 최대한의 신고 데이터를 모두 사용할 수 있게 제공한다\n",
    "    \"\"\"\n",
    "\n",
    "    random_seed=2023\n",
    "\n",
    "    target_cnt = int(round(nrows*target_percent)) # 위해 지표 0이 아닌 데이터의 목표 개수\n",
    "    report_cnt = pop_merge_df.loc[pop_merge_df[target_col] > 0].shape[0] # 위해 지표가 0이 아닌 데이터의 실제 개수\n",
    "    \n",
    "    logging.info(f'신고건수 : {report_cnt}')\n",
    "                 \n",
    "    if report_cnt < target_cnt:\n",
    "        logging.info(f'다운샘플 시 위해지표가 0 이 아닌 데이터가 {target_cnt:,}로 신고가 없는 많이 없는 경우. {report_cnt}')\n",
    "        n1 = report_cnt\n",
    "        n2 = int(((1-target_percent) / target_percent) * report_cnt)\n",
    "        \n",
    "    else:\n",
    "        logging.info(f'{nrows:,} 건 다운샘플 시 위해지표가 0 이 아닌 데이터가 목표 개수 충족하는 경우 경우. ')\n",
    "        n1 = int(nrows * target_percent)\n",
    "        n2 = int(nrows * (1 - target_percent))\n",
    "\n",
    "    sample1 = pop_merge_df.loc[pop_merge_df[target_col] > 0].sample(n=n1, random_state=random_seed) # 신고점수가 0이 아닌 데이터의 비율\n",
    "    sample2 = pop_merge_df.loc[pop_merge_df[target_col] == 0].sample(n=n2, random_state=random_seed) # 신고점수가 0인 데이터의 비율\n",
    "\n",
    "    sampling_df = pd.concat([sample1, sample2], ignore_index=True)\n",
    "    \n",
    "    return sampling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890e951-fe84-45b3-a923-fc66f049406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. 클래스 불균형을 해소하기 위해 신고 발생 · 미발생 데이터를 일정 비율로 조절(위해지표으로만)\n",
    "sampling_df = down_sampling(pop_merge_df, 'hazard_score', nrows=1_000_000, target_percent=0.25)\n",
    "logging.info('-- down_sampling() 종료 --')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379b8bc-5f0f-4318-8c7d-bc09b946348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_df.to_csv(os.path.join(PREPROCESSING_DATA_FOLDER, f'{gu_nm}2.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d87cbb-fc58-48cf-9027-c5031aeb33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_df.isnull().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
